{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1626646,"sourceType":"datasetVersion","datasetId":961513},{"sourceId":2491748,"sourceType":"datasetVersion","datasetId":1500837},{"sourceId":8635095,"sourceType":"datasetVersion","datasetId":5170850},{"sourceId":11205140,"sourceType":"datasetVersion","datasetId":5087711},{"sourceId":179800974,"sourceType":"kernelVersion"},{"sourceId":179841629,"sourceType":"kernelVersion"},{"sourceId":244539235,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n## This Project aims to analize different eXplanable AI techniques. This project can be divided in 3 macro-section.\n1) In the first macro-section, different XAI techniques (i.e., RISE, LIME, Grad-CAM, Grad-CAM++) are applied to an image classification task, followed by the extraction of saliency maps using these various methods.\n2) In the other section, these techniques are compared using different metrics (Insertion, Deletion, Pointing Game), analyzing their respective advantages and disadvantages.\n3) In the last section, an XAI technique is applied in the image classification domain to investigate the presence of biases in the models.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nfrom torchvision import transforms, utils\nfrom torchvision.models import ResNet50_Weights\nfrom torch.nn.functional import conv2d\nimport torch.utils.data\nfrom torch.utils.data import DataLoader, Subset, Dataset\nimport torch.nn.functional as F\n\nimport os\nfrom fastcore.all import *\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\nimport cv2\n\nfrom utils import *\nfrom evaluation import CausalMetric, auc, gkern\nfrom explanation import RISE, RISEBatch\n\nimport PIL\n\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\n\ncudnn.benchmark = True\n\ndevice = 'cuda' if torch.cuda.is_available else 'cpu'\n!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"_uuid":"34d06aef-1d58-4a31-a868-5514106fc176","_cell_guid":"4534f0bd-41fc-4ce9-bbb6-b4bd3ba6d45f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-13T14:38:38.616958Z","iopub.execute_input":"2025-06-13T14:38:38.617569Z","iopub.status.idle":"2025-06-13T14:38:52.444027Z","shell.execute_reply.started":"2025-06-13T14:38:38.617527Z","shell.execute_reply":"2025-06-13T14:38:52.443153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"args = Dummy()\n\n\nargs.workers = 4\n\nargs.datadir = '/kaggle/input/imagenet100/val.X'\n\n# extract a subset of range (100) images to apply the classification model and the eXplainability techniques\nargs.range = range(0, 100)\n\n# Size of imput images\nargs.input_size = (224, 224)\n\nargs.gpu_batch = 10\nargs.batch_size = 10\n\ndataset = datasets.ImageFolder(args.datadir, preprocess)\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=args.batch_size, shuffle=False,\n    num_workers=args.workers, pin_memory=True, sampler=RangeSampler(args.range))\n\nprint('Dataset size: {:}'.format(len(dataset)))\nprint('number of classes: {:}'.format(len(dataset.classes)))\nprint('Selected dataloader size:{: }'.format(len(data_loader) * data_loader.batch_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:39:10.938965Z","iopub.execute_input":"2025-06-13T14:39:10.940293Z","iopub.status.idle":"2025-06-13T14:39:24.232571Z","shell.execute_reply.started":"2025-06-13T14:39:10.940262Z","shell.execute_reply":"2025-06-13T14:39:24.231829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Resnet50 pretrained on the training set of the imageNet dataset\n## In our case, it is necessary to define a neural network output layer with 100 units, because we are working on a subset of ImageNet that contains only 100 out of 1000 classes.","metadata":{}},{"cell_type":"code","source":"model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nmodel = nn.Sequential(model, nn.Softmax(dim=1))\nmodel = model.eval()\nmodel = model.cuda()\n\nfor p in model.parameters():\n    p.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:02:25.090089Z","iopub.execute_input":"2025-06-13T08:02:25.090734Z","iopub.status.idle":"2025-06-13T08:02:26.412607Z","shell.execute_reply.started":"2025-06-13T08:02:25.090707Z","shell.execute_reply":"2025-06-13T08:02:26.411812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SECTION 1: XAI Overview and comparison\n\n## 1) Simple example of a saliency map extracted using the RISE eXplainability technique\n### Note: some of the classes and utility functions are taken from the RISE paper (https://arxiv.org/pdf/1806.07421) and its public code on GitHub (https://github.com/eclique/RISE).","metadata":{}},{"cell_type":"code","source":"explainer = RISE(model, args.input_size, args.gpu_batch)\n\nmaskspath = 'masks.npy'\ngenerate_new = True\n\nif generate_new or not os.path.isfile(maskspath):\n    explainer.generate_masks(N=1000, s=8, p1=0.1, savepath=maskspath)\nelse:\n    explainer.load_masks(maskspath)\n    print('Masks are loaded.')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### This method is extracted from the RISE paper and return a saliency map of the top_k classes (selected 5) predicted of a particular image (default value is 3)","metadata":{}},{"cell_type":"code","source":"def example(img, top_k=3):\n    saliency = explainer(img.cuda()).cpu().numpy()\n    #Model restituisce solo le probabilitÃ  softmax\n    p, c = torch.topk(model(img.cuda()), k=top_k)\n    p, c = p[0], c[0]\n    \n    plt.figure(figsize=(10, 5*top_k))\n    for k in range(top_k):\n        plt.subplot(top_k, 2, 2*k+1)\n        plt.axis('off')\n        plt.title('{:.2f}% {}'.format(100*p[k], get_class_name(c[k])))\n        tensor_imshow(img[0])\n\n        plt.subplot(top_k, 2, 2*k+2)\n        plt.axis('off')\n        plt.title(get_class_name(c[k]))\n        tensor_imshow(img[0])\n        sal = saliency[c[k]]\n        plt.imshow(sal, cmap='jet', alpha=0.5)\n        plt.colorbar(fraction=0.046, pad=0.04)\n\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example(read_tensor('/kaggle/input/imagenet100/val.X/n01443537/ILSVRC2012_val_00000994.JPEG'), 5)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2) Comparision between different eXplainability techniques:\n 1. RISE\n 2. LIME\n 3. GradCAM\n 4. GradCAM++\n\n### 2.0.1) Dataset subset extraction","metadata":{}},{"cell_type":"code","source":"# Dataset creation using Imagefolder\ndataset = datasets.ImageFolder(args.datadir)\n\n# Select a subset using the specified indexes\nsubset_indices = list(args.range)\nsubset = Subset(dataset, subset_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:54:45.346158Z","iopub.execute_input":"2025-06-13T14:54:45.346491Z","iopub.status.idle":"2025-06-13T14:54:50.681776Z","shell.execute_reply.started":"2025-06-13T14:54:45.346458Z","shell.execute_reply":"2025-06-13T14:54:50.681010Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.0.2) GradCAM & GradCAM ++ implementations","metadata":{}},{"cell_type":"code","source":"def find_resnet_layer(arch, target_layer_name):\n    if 'layer' in target_layer_name:\n        hierarchy = target_layer_name.split('_')\n        layer_num = int(hierarchy[0].lstrip('layer'))\n        if layer_num == 1:\n            target_layer = arch.layer1\n        elif layer_num == 2:\n            target_layer = arch.layer2\n        elif layer_num == 3:\n            target_layer = arch.layer3\n        elif layer_num == 4:\n            target_layer = arch.layer4\n        else:\n            raise ValueError('unknown layer : {}'.format(target_layer_name))\n\n        if len(hierarchy) >= 2:\n            bottleneck_num = int(hierarchy[1].lower().lstrip('bottleneck').lstrip('basicblock'))\n            target_layer = target_layer[bottleneck_num]\n\n        if len(hierarchy) >= 3:\n            target_layer = target_layer._modules[hierarchy[2]]\n                \n        if len(hierarchy) == 4:\n            target_layer = target_layer._modules[hierarchy[3]]\n\n    else:\n        target_layer = arch._modules[target_layer_name]\n\n    return target_layer\n\ndef visualize_cam(mask, img):\n    \"\"\"Make heatmap from mask and synthesize GradCAM result image using heatmap and img.\n    Args:\n        mask (torch.tensor): mask shape of (1, 1, H, W) and each element has value in range [0, 1]\n        img (torch.tensor): img shape of (1, 3, H, W) and each pixel value is in range [0, 1]\n        \n    Return:\n        heatmap (torch.tensor): heatmap img shape of (3, H, W)\n        result (torch.tensor): synthesized GradCAM result of same shape with heatmap.\n    \"\"\"\n    \n    heatmap = cv2.applyColorMap(np.uint8(255 * mask.squeeze()), cv2.COLORMAP_JET)\n    heatmap = torch.from_numpy(heatmap).permute(2, 0, 1).float().div(255)\n    b, g, r = heatmap.split(1)\n    heatmap = torch.cat([r, g, b])\n    \n    result = heatmap+img.cpu()\n    result = result.div(result.max()).squeeze()\n    \n    return heatmap, result\n\ndef denormalize(tensor, mean, std):\n    if not tensor.ndimension() == 4:\n        raise TypeError('tensor should be 4D')\n\n    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n\n    return tensor.mul(std).add(mean)\n\n\ndef normalize(tensor, mean, std):\n    if not tensor.ndimension() == 4:\n        raise TypeError('tensor should be 4D')\n\n    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n\n    return tensor.sub(mean).div(std)\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        return self.do(tensor)\n    \n    def do(self, tensor):\n        return normalize(tensor, self.mean, self.std)\n    \n    def undo(self, tensor):\n        return denormalize(tensor, self.mean, self.std)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:54:53.694035Z","iopub.execute_input":"2025-06-13T14:54:53.694341Z","iopub.status.idle":"2025-06-13T14:54:53.706076Z","shell.execute_reply.started":"2025-06-13T14:54:53.694320Z","shell.execute_reply":"2025-06-13T14:54:53.705326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nclass GradCAM(object):\n    def __init__(self, model_dict, verbose=False):\n        model_type = model_dict['type']\n        layer_name = model_dict['layer_name']\n        self.model_arch = model_dict['arch']\n\n        self.gradients = dict()\n        self.activations = dict()\n        def backward_hook(module, grad_input, grad_output):\n            self.gradients['value'] = grad_output[0]\n            return None\n        def forward_hook(module, input, output):\n            self.activations['value'] = output\n            return None\n\n        if 'vgg' in model_type.lower():\n            target_layer = find_vgg_layer(self.model_arch, layer_name)\n        elif 'resnet' in model_type.lower():\n            target_layer = find_resnet_layer(self.model_arch, layer_name)\n        elif 'densenet' in model_type.lower():\n            target_layer = find_densenet_layer(self.model_arch, layer_name)\n        elif 'alexnet' in model_type.lower():\n            target_layer = find_alexnet_layer(self.model_arch, layer_name)\n        elif 'squeezenet' in model_type.lower():\n            target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n\n        target_layer.register_forward_hook(forward_hook)\n        target_layer.register_full_backward_hook(backward_hook)\n\n        if verbose:\n            try:\n                input_size = model_dict['input_size']\n            except KeyError:\n                print(\"please specify size of input image in model_dict. e.g. {'input_size':(224, 224)}\")\n                pass\n            else:\n                device = 'cuda' if next(self.model_arch.parameters()).is_cuda else 'cpu'\n                self.model_arch(torch.zeros(1, 3, *(input_size), device=device))\n                print('saliency_map size :', self.activations['value'].shape[2:])\n\n\n    def forward(self, input, class_idx=None, retain_graph=False):\n        \"\"\"\n        Args:\n            input: input image with shape of (1, 3, H, W)\n            class_idx (int): class index for calculating GradCAM.\n                    If not specified, the class index that makes the highest model prediction score will be used.\n        Return:\n            mask: saliency map of the same spatial dimension with input\n            logit: model output\n        \"\"\"\n        b, c, h, w = input.size()\n\n        logit = self.model_arch(input)\n        if class_idx is None:\n            score = logit[:, logit.max(1)[-1]].squeeze()\n        else:\n            score = logit[:, class_idx].squeeze()\n\n        self.model_arch.zero_grad()\n        score.backward(retain_graph=retain_graph)\n        gradients = self.gradients['value']\n        activations = self.activations['value']\n        b, k, u, v = gradients.size()\n\n        alpha = gradients.view(b, k, -1).mean(2)\n        #alpha = F.relu(gradients.view(b, k, -1)).mean(2)\n        weights = alpha.view(b, k, 1, 1)\n\n        saliency_map = (weights*activations).sum(1, keepdim=True)\n        saliency_map = F.relu(saliency_map)\n        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n\n        return saliency_map, logit\n\n    def __call__(self, input, class_idx=None, retain_graph=False):\n        return self.forward(input, class_idx, retain_graph)\n    \nclass GradCAMpp(GradCAM):\n\n    \"\"\"\n    Args:\n        model_dict (dict): a dictionary that contains 'model_type', 'arch', layer_name', 'input_size'(optional) as keys.\n        verbose (bool): whether to print output size of the saliency map givien 'layer_name' and 'input_size' in model_dict.\n    \"\"\"\n    def __init__(self, model_dict, verbose=False):\n        super(GradCAMpp, self).__init__(model_dict, verbose)\n\n    def forward(self, input, class_idx=None, retain_graph=False):\n        \"\"\"\n        Args:\n            input: input image with shape of (1, 3, H, W)\n            class_idx (int): class index for calculating GradCAM.\n                    If not specified, the class index that makes the highest model prediction score will be used.\n        Return:\n            mask: saliency map of the same spatial dimension with input\n            logit: model output\n        \"\"\"\n        b, c, h, w = input.size()\n\n        logit = self.model_arch(input)\n        if class_idx is None:\n            score = logit[:, logit.max(1)[-1]].squeeze()\n        else:\n            score = logit[:, class_idx].squeeze() \n            \n        self.model_arch.zero_grad()\n        score.backward(retain_graph=retain_graph)\n        gradients = self.gradients['value'] # dS/dA\n        activations = self.activations['value'] # A\n        b, k, u, v = gradients.size()\n\n        alpha_num = gradients.pow(2)\n        alpha_denom = gradients.pow(2).mul(2) + \\\n                activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n\n        alpha = alpha_num.div(alpha_denom+1e-7)\n        positive_gradients = F.relu(score.exp()*gradients) # ReLU(dY/dA) == ReLU(exp(S)*dS/dA))\n        weights = (alpha*positive_gradients).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n\n        saliency_map = (weights*activations).sum(1, keepdim=True)\n        saliency_map = F.relu(saliency_map)\n        saliency_map = F.interpolate(saliency_map, size=(224, 224), mode='bilinear', align_corners=False)\n        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n        saliency_map = (saliency_map-saliency_map_min).div(saliency_map_max-saliency_map_min).data\n        \n        #print(\"Saliency Dimension\")\n        #print(saliency_map.shape)\n\n        return saliency_map, logit\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:54:58.073965Z","iopub.execute_input":"2025-06-13T14:54:58.074512Z","iopub.status.idle":"2025-06-13T14:54:58.089671Z","shell.execute_reply.started":"2025-06-13T14:54:58.074485Z","shell.execute_reply":"2025-06-13T14:54:58.088824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This is a Resnet50 pretrained on ImageNet, but without the final softmax layer\n#This is necessary for the GradCAM and GradCAM++ non model-agnostic techniques\nresnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nresnet.eval()\nresnet = resnet.cuda()\n\nresnet_model_dict = dict(type='resnet', arch=resnet, layer_name='layer4', input_size=(224, 224))\nresnet_gradcam = GradCAM(resnet_model_dict, False)\nresnet_gradcampp = GradCAMpp(resnet_model_dict, False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.1.1) eXplain with GCAM","metadata":{}},{"cell_type":"code","source":"    gradCAMExplanations = np.empty((len(subset), *args.input_size,3))\n    gradCAMppExplanations = np.empty((len(subset), *args.input_size,3))\n    heatCAMExplanations = np.empty((len(subset), *args.input_size))\n    heatCAMppExplanations = np.empty((len(subset), *args.input_size))\n\n    #maskCAM = np.empty((len(data_loader), *args.input_size))\n    #arrayImg = []\n\n    for i, (img, _) in enumerate(tqdm(subset, total=len(subset), desc='Explaining images with GRADCAM e GRADCAM++')):\n\n        normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        np_img = np.asarray(img).copy()\n        torch_img = torch.from_numpy(np_img).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\n        torch_img = F.interpolate(torch_img, size=(224, 224), mode='bilinear', align_corners=False)\n        normed_torch_img = normalizer(torch_img)\n\n        mask, logits = resnet_gradcam(normed_torch_img)\n        heatmap, result = visualize_cam(mask.cpu(), torch_img.cpu())\n\n        #print(heatmap.shape)\n        #print(mask.shape)\n\n        # Aggiunta di result e Maschera all'array delle predizioni\n        # Adding result and mask to the prediction array\n        heatCAMExplanations[i] = mask[0][0].cpu()#.numpy()\n        gradCAMExplanations[i] = result.permute(1, 2, 0).numpy()\n\n        #maskCAM[i] = mask[0][0].cpu().permute(1, 2, 0).cpu().numpy()\n        #arrayImg.append(result)\n\n        mask_pp, _ = resnet_gradcampp(normed_torch_img)\n        heatmap_pp, result_pp = visualize_cam(mask_pp.cpu(), torch_img.cpu())\n\n\n        gradCAMppExplanations[i] = result_pp.permute(1, 2, 0)\n        heatCAMppExplanations[i] = mask_pp[0][0].cpu()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##Save GCAM explanations\nheatCAMExplanations.tofile('gradCAMExp_{:05}-{:05}.npy'.format(args.range[0], args.range[-1]))\n\n##Save GCAM++ explanations\nheatCAMppExplanations.tofile('gradCAMppExp_{:05}-{:05}.npy'.format(args.range[0], args.range[-1]))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2) eXplain Batch using RISE technique","metadata":{}},{"cell_type":"code","source":"def explain_all_batch(data_loader, explainer):\n    n_batch = len(data_loader)\n    b_size = data_loader.batch_size\n    total = n_batch * b_size\n    print(f\"batch_number {n_batch}\")\n    print(f\"batch_size {b_size}\")\n#     # Get all predicted labels first\n\n    target = np.empty(total, 'int64')\n    for i, (imgs, _) in enumerate(tqdm(data_loader, total=n_batch, desc='Predicting labels')):\n        p, c = torch.max(explainer.model(imgs.cuda()), dim=1)\n        target[i * b_size:(i + 1) * b_size] = c.cpu()\n    image_size = imgs.shape[-2:]\n\n#     # Get saliency maps for all images in val loader\n\n    explanations = np.empty((total, *image_size))\n    for i, (imgs, _) in enumerate(tqdm(data_loader, total=n_batch, desc='Explaining images')):\n        saliency_maps = explainer(imgs.cuda())\n        explanations[i * b_size:(i + 1) * b_size] = saliency_maps[\n            range(b_size), target[i * b_size:(i + 1) * b_size]].data.cpu().numpy()\n    return explanations","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The N parameter represents the number of perturbated input image (masks)\n### The s parameter represent the stride (i.e. the number of the mask dimension that will be later upscaled to perturbe the image)\n### p1 is the probability to keep a pixel","metadata":{}},{"cell_type":"code","source":"explainer = RISEBatch(model, args.input_size, args.gpu_batch)\n\n# Explain all the batches defined in the dataloader. It's possible to set a higher value of masks \nexplainer.generate_masks(N=1000, s=7, p1=0.5)\nexplanations_batch_rise = explain_all_batch(data_loader, explainer)\n\n#Save the explanations in a file\nexplanations_batch_rise.tofile('riseExp_{:05}-{:05}.npy'.format(args.range[0], args.range[-1]))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3) eXplain Batch using LIME technique","metadata":{}},{"cell_type":"code","source":"    # Get all predicted labels first\n    target = np.empty(len(data_loader), np.int32)\n    for i, (img, _) in enumerate(tqdm(data_loader, total=len(data_loader), desc='Predicting labels')):\n        p, c = torch.max(model(img.cuda()), dim=1)\n        target[i] = c[0]\n        \n    lime_explainer = lime_image.LimeImageExplainer()\n    heatmapLime = np.empty((len(data_loader) * args.batch_size, *args.input_size))\n    limeExplanations = np.empty((len(data_loader) * args.batch_size, *args.input_size,3))\n\n    img_idx = 0  \n    for batch_idx, (imgs, _) in enumerate(tqdm(data_loader, total=len(data_loader), desc='Explaining images with LIME')):\n        batch_size = imgs.shape[0]  \n        \n        for i in range(batch_size):\n            img = imgs[i].permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C) format for LIME\n            \n            def batch_predict(images):\n                images = torch.stack([torch.tensor(image).permute(2, 0, 1) for image in images]).float()\n                images = images.cuda()\n                probs = model(images)\n                probs = probs.cpu().detach().numpy()\n                return probs\n            \n            # Predizione per l'immagine corrente\n            prediction = batch_predict(img.reshape(1, *img.shape))\n            predicted_class = np.argmax(prediction[0])\n            \n            explanation = lime_explainer.explain_instance(\n                img, batch_predict, \n                top_labels=1, \n                hide_color=0, \n                num_samples=1000\n            )\n            \n            temp, mask = explanation.get_image_and_mask(\n                predicted_class, \n                positive_only=True, \n                num_features=5, \n                hide_rest=False\n            )\n            \n            limeExplanations[img_idx] = mark_boundaries(temp / 255.0, mask)\n            \n            # Map each explanation weight to the corresponding superpixel\n            dict_heatmap = dict(explanation.local_exp[predicted_class])\n            heatmap = np.vectorize(dict_heatmap.get)(explanation.segments)\n            heatmapLime[img_idx] = heatmap\n            \n            img_idx += 1  # Incrementa l'indice globale","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the explanations in a numpy format\nval_data_ini = args.range[0]\nval_data_fin = args.range[-1]\nstart_index = args.range[0]\nend_index = args.range[-1]\n\n# Save the heatmapLime\nheatmapLime_subset = heatmapLime[start_index:end_index + 1] \nnp.save('limeExp_{:05}-{:05}.npy'.format(val_data_ini, val_data_fin), heatmapLime_subset)\n\n# Save the LimeExplanations\nlimeExplanations_subset = limeExplanations[start_index:end_index + 1]  \nnp.save('limeMaskExp_{:05}-{:05}.npy'.format(val_data_ini, val_data_fin), limeExplanations_subset)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.4) eXplainAll Dataloader method","metadata":{}},{"cell_type":"code","source":"## in this case the explainer must be RISEBatch\nexplainer = RISEBatch(model, args.input_size, args.gpu_batch)\n\n# Explain all the batches defined in the dataloader. It's possible to set a higher value of masks \nexplainer.generate_masks(N=1000, s=7, p1=0.5)\n\ndef explain_all(data_loader, explainer):\n\n    n_batch = len(data_loader)\n    b_size = data_loader.batch_size\n    total = n_batch * b_size\n    print(f\"batch_number {n_batch}\")\n    print(f\"batch_size {b_size}\")\n#     # Get all predicted labels first\n\n    target = np.empty(total, 'int64')\n    for i, (imgs, _) in enumerate(tqdm(data_loader, total=n_batch, desc='Predicting labels')):\n        p, c = torch.max(explainer.model(imgs.cuda()), dim=1)\n        target[i * b_size:(i + 1) * b_size] = c.cpu()\n    image_size = imgs.shape[-2:]\n\n#     # Get saliency maps for all images in val loader\n\n    riseExplanations = np.empty((total, *image_size))\n    for i, (imgs, _) in enumerate(tqdm(data_loader, total=n_batch, desc='Explaining images')):\n        saliency_maps = explainer(imgs.cuda())\n        riseExplanations[i * b_size:(i + 1) * b_size] = saliency_maps[\n            range(b_size), target[i * b_size:(i + 1) * b_size]].data.cpu().numpy()\n    riseExplanations = riseExplanations/255\n\n    \n    lime_explainer = lime_image.LimeImageExplainer()\n    # Calcola il numero totale di immagini\n    total_images = len(data_loader) * data_loader.batch_size\n    heatmapLime = np.empty((total_images, *args.input_size))\n    limeExplanations = np.empty((total_images, *args.input_size, 3))\n    \n    img_idx = 0  \n    \n    for batch_idx, (imgs, _) in enumerate(tqdm(data_loader, total=len(data_loader), desc='Explaining images with LIME')):\n        batch_size = imgs.shape[0] \n        \n        for i in range(batch_size):\n            img = imgs[i].permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C) format for LIME\n            \n            def batch_predict(images):\n                images = torch.stack([torch.tensor(image).permute(2, 0, 1) for image in images]).float()\n                images = images.cuda()\n                probs = model(images)\n                probs = probs.cpu().detach().numpy()\n                return probs\n            \n            # Predizione per l'immagine corrente\n            prediction = batch_predict(img.reshape(1, *img.shape))\n            predicted_class = np.argmax(prediction[0])\n            \n            explanation = lime_explainer.explain_instance(\n                img, batch_predict, \n                top_labels=1, \n                hide_color=0, \n                num_samples=1000\n            )\n            \n            temp, mask = explanation.get_image_and_mask(\n                predicted_class, \n                positive_only=True, \n                num_features=5, \n                hide_rest=False\n            )\n            \n            limeExplanations[img_idx] = mark_boundaries(temp / 255.0, mask)\n            \n            # Map each explanation weight to the corresponding superpixel\n            dict_heatmap = dict(explanation.local_exp[predicted_class])\n            heatmap = np.vectorize(dict_heatmap.get)(explanation.segments)\n            heatmapLime[img_idx] = heatmap\n            \n            img_idx += 1  # Incrementa l'indice globale\n    \n    gradCAMExplanations = np.empty((len(subset), *args.input_size,3))\n    gradCAMppExplanations = np.empty((len(subset), *args.input_size,3))\n    heatCAMExplanations = np.empty((len(subset), *args.input_size))\n    heatCAMppExplanations = np.empty((len(subset), *args.input_size))\n\n    #maskCAM = np.empty((len(data_loader), *args.input_size))\n    #arrayImg = []\n\n    for i, (img, _) in enumerate(tqdm(subset, total=len(subset), desc='Explaining images with GRADCAM e GRADCAM++')):\n\n        normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        np_img = np.asarray(img).copy()\n        torch_img = torch.from_numpy(np_img).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\n        torch_img = F.interpolate(torch_img, size=(224, 224), mode='bilinear', align_corners=False)\n        normed_torch_img = normalizer(torch_img)\n\n        mask, logits = resnet_gradcam(normed_torch_img)\n        heatmap, result = visualize_cam(mask.cpu(), torch_img.cpu())\n\n        #print(heatmap.shape)\n        #print(mask.shape)\n\n        # Aggiunta di result e Maschera all'array delle predizioni \n        heatCAMExplanations[i] = mask[0][0].cpu()#.numpy()\n        gradCAMExplanations[i] = result.permute(1, 2, 0).numpy()\n\n        #maskCAM[i] = mask[0][0].cpu().permute(1, 2, 0).cpu().numpy()\n        #arrayImg.append(result)\n\n        mask_pp, _ = resnet_gradcampp(normed_torch_img)\n        heatmap_pp, result_pp = visualize_cam(mask_pp.cpu(), torch_img.cpu())\n\n\n        gradCAMppExplanations[i] = result_pp.permute(1, 2, 0)\n        heatCAMppExplanations[i] = mask_pp[0][0].cpu()\n    \n    \n    return riseExplanations , limeExplanations, heatCAMExplanations, heatCAMppExplanations, heatmapLime","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"riseExplanations, limeExplanations, gradCAMExplanations, gradCAMppExplanations, heatmapLime = explain_all(data_loader, explainer)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save the explanations in a numpy array file","metadata":{}},{"cell_type":"code","source":"riseExplanations.tofile('riseExp_{:05}-{:05}.npy'.format(args.range[0], args.range[-1]))\n\nheatmapLime.tofile('limeExp_{:05}-{:05}.npy'.format(args.range[0], args.range[-1]))\n\ngradCAMExplanations.tofile('gradCAMExp_{:05}-{:05}.npy'.format(args.range[0], args.range[-1]))\n\ngradCAMppExplanations.tofile('gradCAMppExp_{:05}-{:05}.npy'.format(args.range[0], args.range[-1]))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 3) Visual comparision between the 4 different techniques","metadata":{}},{"cell_type":"code","source":"# Load all the arrays\nrise_heatmaps = np.fromfile('riseExp_00000-00099.npy').reshape(args.range[-1]+1,224,224)\nlime_heatmaps = np.fromfile('limeExp_00000-00099.npy').reshape(args.range[-1]+1,224,224)\ngradcam_heatmaps = np.fromfile('gradCAMExp_00000-00099.npy').reshape(args.range[-1]+1,224,224)\ngradcampp_heatmaps = np.fromfile('gradCAMppExp_00000-00099.npy').reshape(args.range[-1]+1,224,224)\n\n# Load the image\nimg = read_tensor('/kaggle/input/imagenet100/val.X/n01443537/ILSVRC2012_val_00000262.JPEG')\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('Heatmaps comparison', fontsize=16, fontweight='bold')\n\nimg_idx = 51\n\nimg_np = img[0].permute(1, 2, 0).cpu().numpy()\n\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\nimg_denorm = img_np * std + mean\nimg_denorm = np.clip(img_denorm, 0, 1)\n\naxes[0, 0].imshow(img_denorm)\nim1 = axes[0, 0].imshow(gradcam_heatmaps[img_idx], cmap='jet', alpha=0.5)\naxes[0, 0].set_title('GradCAM Heatmap')\naxes[0, 0].axis('off')\n\naxes[0, 1].imshow(img_denorm)\nim2 = axes[0, 1].imshow(gradcampp_heatmaps[img_idx], cmap='jet', alpha=0.5)\naxes[0, 1].set_title('GradCAMpp Heatmap')\naxes[0, 1].axis('off')\n\n\naxes[1, 0].imshow(img_denorm)\nim3 = axes[1, 0].imshow(rise_heatmaps[img_idx], cmap='jet', alpha=0.5)\naxes[1, 0].set_title('RISE Heatmap')\naxes[1, 0].axis('off')\n\naxes[1, 1].imshow(img_denorm)\nim4 = axes[1, 1].imshow(lime_heatmaps[img_idx], cmap='jet', alpha=0.5)\naxes[1, 1].set_title('LIME Heatmap')\naxes[1, 1].axis('off')\n\n\n# Aggiungi colorbar condivisa\ncbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\ncbar = fig.colorbar(im1, cax=cbar_ax)\ncbar.set_label('Importanza', rotation=270, labelpad=20)\n\nplt.tight_layout()\nplt.subplots_adjust(right=0.9)\nplt.show()\n\n# Debug print to verify the heatmap dimensions\nprint(f\"Shape rise_heatmaps: {rise_heatmaps.shape}\")\nprint(f\"Shape lime_heatmaps: {lime_heatmaps.shape}\")\nprint(f\"Shape gradcam_heatmaps: {gradcam_heatmaps.shape}\")\nprint(f\"Shape gradcampp_heatmaps: {gradcampp_heatmaps.shape}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 2: Metric presentation and comparison between the different techniques\n## 4) Test the various technique using the deletion/insertion metrics and the pointing game","metadata":{}},{"cell_type":"markdown","source":"1. Deletion\n\nAUC close to 0: Indicates that by removing the important pixels, the model's prediction degrades rapidly. This means that the removed pixels were crucial for the modelâs correct prediction. A smaller area under the curve is desirable, as it suggests that the identified pixels are indeed important.\n\nAUC close to 1: Indicates that by removing the pixels, the modelâs prediction does not degrade rapidly, suggesting that the removed pixels were not very important for the prediction. A larger area under the curve is less desirable, as it suggests that the removed pixels were not critical.\n\n2. Insertion\n\nAUC close to 1: Indicates that by inserting the important pixels, the modelâs prediction improves rapidly. This means that the added pixels were very informative and important for the modelâs correct prediction. A larger area under the curve is desirable, as it suggests that the identified pixels are indeed informative.\n\nAUC close to 0: Indicates that by inserting the pixels, the modelâs prediction does not improve very rapidly, suggesting that the added pixels were not very informative. A smaller area under the curve is less desirable, as it suggests that the added pixels were not critical.","metadata":{}},{"cell_type":"markdown","source":"## 4.1) Visual explanations of the two metrics","metadata":{}},{"cell_type":"code","source":"klen = 11\nksig = 5\nkern = gkern(klen, ksig)\n\n\n# Function that blurs input image\nblur = lambda x: nn.functional.conv2d(x, kern, padding=klen//2)\n\ndef random_noise(image):\n    noise = torch.randn_like(image)\n    return noise\n\ndef white_pixels(image):\n    # Assuming the image is an 8-bit image, so the white pixel value is 255\n    return torch.ones_like(image) * 255\n\n#The step parameter select the number of pixel substituted during each iteration.\n\ninsertion = CausalMetric(model, 'ins', step = 896, substrate_fn=torch.zeros_like)\ndeletion = CausalMetric(model, 'del', step = 896, substrate_fn=torch.zeros_like)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:40:13.991595Z","iopub.execute_input":"2025-06-13T14:40:13.992246Z","iopub.status.idle":"2025-06-13T14:40:14.058741Z","shell.execute_reply.started":"2025-06-13T14:40:13.992220Z","shell.execute_reply":"2025-06-13T14:40:14.057858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplt.subplot(131)\nplt.axis('off')\nimg = read_tensor('/kaggle/input/imagenet100/val.X/n01443537/ILSVRC2012_val_00000236.JPEG')\ntensor_imshow(img[0])\n\nsal = rise_heatmaps[50]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"h = deletion.single_run(img, sal, verbose=2)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"h = insertion.single_run(img, sal, verbose=2)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4.2) Deletion and insertion test on a set of images\n\n    Args of the evaluate method\n        \"\"\" Efficiently evaluate big batch of images.\n\n        Args:\n            img_batch (Tensor): batch of images.\n            exp_batch (np.ndarray): batch of explanations.\n            batch_size (int): number of images for one small batch.\n\n        Returns:\n            scores (nd.array): Array containing scores at every step for every image.\n        \"\"\"","metadata":{}},{"cell_type":"code","source":"scores = {'del': [], 'ins': []}\n\nimages = np.empty((len(data_loader), args.batch_size, 3, 224, 224))\nfor j, (img, _) in enumerate(tqdm(data_loader, total=len(data_loader), desc='Loading images')):\n    images[j] = img\nimages = images.reshape((-1, 3, 224, 224))\n\n# Load the saliency map\nexp = np.fromfile('/kaggle/working/riseExp_00000-00099.npy').reshape(len(args.range), 224, 224)\n#exp = result\n\n#(h.mean(1) Calcola la media delle probabilitÃ  che appartenga alla classe target per ogni step di cancellazione\nh = deletion.evaluate(torch.from_numpy(images.astype('float32')), exp, 10)\nscores['del'].append(auc(h.mean(1)))\n\n#Insertion\nh = insertion.evaluate(torch.from_numpy(images.astype('float32')), exp, 10)\nscores['ins'].append(auc(h.mean(1)))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2.1) Considerations on the method\n\n### The CasualMetric adopted in this method gives some problems:\n1) They're dependent on the substrate function adopted\n2) They calculate the AUC score for the most probable class, and it doesn't use the ground truth label","metadata":{}},{"cell_type":"markdown","source":"## 4.3) Pointing Game","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\n\n# Define the paths\nvoc_root = '/kaggle/input/pascal-voc-2007-and-2012/VOCdevkit/VOC2007'  \nannotations_dir = os.path.join(voc_root, 'SegmentationClass')\noutput_dir = '/kaggle/working/masks3' \nos.makedirs(output_dir, exist_ok=True)\n\nclass_names = [\n    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n    \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n    \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n]\n\ndef extract_class_masks(annotation_path, output_path):\n    \n    annotation = Image.open(annotation_path)\n    annotation_np = np.array(annotation)\n\n    # Extract the classes\n    present_classes = np.unique(annotation_np)\n    present_classes = [class_index for class_index in present_classes if class_index < len(class_names)]\n\n    #print(f\"Processing {os.path.basename(annotation_path)}. Present classes: {present_classes}\")\n\n    # Extract the mask for each class\n    for class_index in present_classes:\n        class_name = class_names[class_index]\n\n        class_mask = (annotation_np == class_index).astype(np.uint8) * 255\n\n        class_mask_img = Image.fromarray(class_mask)\n\n        base_name = os.path.basename(annotation_path).replace('.png', '')\n        output_file = os.path.join(output_path, f\"{base_name}_{class_name}.png\")\n\n        class_mask_img.save(output_file)\n        \n        #print(f\"Saved mask for class {class_name} to {output_file}\")\n\n# Iteration on PASCAL VOC 07\nfor annotation_file in os.listdir(annotations_dir):\n    if annotation_file.endswith('.png'):\n        annotation_path = os.path.join(annotations_dir, annotation_file)\n        extract_class_masks(annotation_path, output_dir)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:40:22.199199Z","iopub.execute_input":"2025-06-13T14:40:22.199538Z","iopub.status.idle":"2025-06-13T14:40:32.498811Z","shell.execute_reply.started":"2025-06-13T14:40:22.199511Z","shell.execute_reply":"2025-06-13T14:40:32.498272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3.1) Creation of a dictionary (image_id -> [classes])","metadata":{}},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\n\ndef parse_voc_annotation(xml_file):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    \n    annotations = set()\n    for obj in root.findall('object'):\n        name = obj.find('name').text\n        annotations.add(name)\n    return list(annotations)\n\ndef create_image_class_dict(annotation_dir, image_dir):\n    image_class_dict = {}\n    \n    for xml_file in os.listdir(annotation_dir):\n        if xml_file.endswith('.xml'):\n            image_id = os.path.splitext(xml_file)[0]\n            image_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n            annotation_path = os.path.join(annotation_dir, xml_file)\n            \n            classes = parse_voc_annotation(annotation_path)\n            image_class_dict[image_id] = classes\n    \n    return image_class_dict\n\nannotation_dir = '/kaggle/input/pascal-voc-2007-and-2012/VOCdevkit/VOC2007/Annotations'  # Directory contenente i file XML delle annotazioni\nimage_dir = '/kaggle/input/pascal-voc-2007-and-2012/VOCdevkit/VOC2007/JPEGImages'  # Directory contenente le immagini\nimage_class_dict = create_image_class_dict(annotation_dir, image_dir)\n\n# Method to print the dict\n#for image_path, classes in image_class_dict.items():\n    #print(f\"{image_path}: {classes}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:42:23.730315Z","iopub.execute_input":"2025-06-13T14:42:23.730622Z","iopub.status.idle":"2025-06-13T14:43:42.095640Z","shell.execute_reply.started":"2025-06-13T14:42:23.730600Z","shell.execute_reply":"2025-06-13T14:43:42.095023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3.2) Train/Validation/Test split of indexes for the images that have segmentation mask of the labels","metadata":{}},{"cell_type":"code","source":"import os\nimport random\n\n#Read images indexes\ndef read_indices(file_path):\n    with open(file_path, 'r') as file:\n        indices = file.readlines()\n    indices = [index.strip() for index in indices]\n    return indices\n\ndef return_dict(segmentation_path,train_file,validation_file,test_file):\n    image_set_dict = {}\n    \n    train = []\n    validation = []\n    test = []\n    \n    #Training set indexes creation\n    train_path = os.path.join(segmentation_path,train_file)\n    train = read_indices(train_path)\n    image_set_dict['train'] = train\n    \n    #Validation set indexes creation\n    validation_path = os.path.join(segmentation_path,validation_file)\n    validation = read_indices(train_path)\n    image_set_dict['validation'] = validation\n    \n    #Tesst set indexes creation\n    test_path = os.path.join(segmentation_path,test_file)\n    test = read_indices(test_path)\n    image_set_dict['test'] = test\n    \n    return image_set_dict\n\nsegmentation_path = '/kaggle/input/pascal-voc-2007-and-2012/VOCdevkit/VOC2007/ImageSets/Segmentation/'\ntrain_file = 'train.txt'\nvalidation_file = 'val.txt'\ntest_file = 'test.txt'\n\nimage_set_dict = return_dict(segmentation_path,train_file,validation_file,test_file)\n#image_set_dict[0:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:45:31.895392Z","iopub.execute_input":"2025-06-13T14:45:31.895915Z","iopub.status.idle":"2025-06-13T14:45:31.905630Z","shell.execute_reply.started":"2025-06-13T14:45:31.895893Z","shell.execute_reply":"2025-06-13T14:45:31.904897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3.3) Image and Mask visualization","metadata":{}},{"cell_type":"markdown","source":"### Utility functions","metadata":{}},{"cell_type":"code","source":"img_folder = '/kaggle/input/pascal-voc-2007-and-2012/VOCdevkit/VOC2007/JPEGImages'\nmask_folder = '/kaggle/input/classesmasksvoc07/kaggle/working/masks3'\n# Extract this image_id\nimg_id = '009901'\n\nn_pixel = 5\npoints = 0\ncont = 0\n\ndef read_segmentation_mask(image_path,target_size=(224, 224)):\n    transform = transforms.Compose([\n        transforms.Resize(target_size),\n        transforms.ToTensor()\n        \n        # for gray-scale images only\n        #transforms.Normalize(mean=[0.5], std=[0.5]) \n    ])\n    \n    # Load the grey-scale image\n    #image = Image.open(image_path).convert(\"L\")\n    \n    return transform(image)\n\n#Mask Binarization\ndef binarize_mask(mask):\n    return mask.int()\n\ndef get_top_salient_pixels(saliency_map, top_k=10):\n    \n    flattened_saliency = saliency_map.flatten()\n    \n    top_k_indices = np.argpartition(flattened_saliency, -top_k)[-top_k:]\n    \n    top_k_indices_2d = np.unravel_index(top_k_indices, saliency_map.shape)\n    return list(zip(top_k_indices_2d[0], top_k_indices_2d[1]))\n\ndef batch_predict(images):\n            images = torch.stack([torch.tensor(image).permute(2, 0, 1) for image in images]).float()\n            images = images.cuda()\n            probs = model(images)\n            probs = probs.cpu().detach().numpy()\n    \n            #If the model hasn't the last classification layer:\n            #probs = torch.nn.functional.softmax(logits, dim=1).cpu().detach().numpy()\n            return probs\n\ndef get_all_predicted_index(classes):\n    ret = []\n    for c in classes:\n        ret.append(voc_class_names.index(c))\n    return ret\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:45:24.455848Z","iopub.execute_input":"2025-06-13T14:45:24.456170Z","iopub.status.idle":"2025-06-13T14:45:24.463205Z","shell.execute_reply.started":"2025-06-13T14:45:24.456145Z","shell.execute_reply":"2025-06-13T14:45:24.462448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = image_class_dict[img_id]\nselected_class = classes[2]\n\nmask_image_path = os.path.join(mask_folder,f'{img_id}_{selected_class}.png')\noriginal_image_path = os.path.join(img_folder, f'{img_id}.jpg')\n\nimage = Image.open(mask_image_path).convert(\"L\")\nmask_image = read_segmentation_mask(image)\n\nprint(\"The class index is\", class_names.index(selected_class))\nprint(\"The class is: \" + selected_class)\n\nimage = Image.open(original_image_path).resize((224,224))\nplt.imshow(np.array(image))\nplt.title('Mask Image')\nplt.axis('off')\nplt.show()\n\nbinary_mask = binarize_mask(mask_image)\ntensor_imshow(binary_mask)\n#tensor_imshow()\n\n#set(mask_image.numpy().flatten())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:46:50.622903Z","iopub.execute_input":"2025-06-13T14:46:50.623578Z","iopub.status.idle":"2025-06-13T14:46:50.994624Z","shell.execute_reply.started":"2025-06-13T14:46:50.623546Z","shell.execute_reply":"2025-06-13T14:46:50.993792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3.4) Multilabel image classificator training","metadata":{}},{"cell_type":"code","source":"path = untar_data(URLs.PASCAL_2007)\n\ndf = pd.read_csv(path/'train.csv')\nfiltered_df = df[df['labels'].str.contains('horse', na=False)]\nonlyHorses = df[df['labels'] == 'horse']\n#filtered_df.head(60)\ndf_test = pd.read_csv(path/'test.csv')\n\nvoc_class_names = [\n     \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n    \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n    \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n]\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\n\ndls = dblock.dataloaders(df)\n\ndls.show_batch(nrows=3, ncols=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:47:06.322938Z","iopub.execute_input":"2025-06-13T14:47:06.323230Z","iopub.status.idle":"2025-06-13T14:48:53.395108Z","shell.execute_reply.started":"2025-06-13T14:47:06.323209Z","shell.execute_reply":"2025-06-13T14:48:53.394267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights = ResNet50_Weights.DEFAULT\nlearn = vision_learner(dls, models.resnet50,weights=weights, metrics=partial(accuracy_multi, thresh=0.1))\nlearn.fine_tune(15, base_lr=3e-3, freeze_epochs=4)\n\nlearn.show_results()\nmodel = learn.model\nmodel.to(device)\nmodel.eval()\n\n# Move the model to CUDA if available\nif torch.cuda.is_available():\n    model = model.cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:50:14.009756Z","iopub.execute_input":"2025-06-13T14:50:14.010465Z","iopub.status.idle":"2025-06-13T14:54:24.031666Z","shell.execute_reply.started":"2025-06-13T14:50:14.010436Z","shell.execute_reply":"2025-06-13T14:54:24.030770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3.5) Visual explanation of Pointing game using GradCAM\n### Note: in this case it's necessary to run previous code cells  at point 2.0 (GCAM)","metadata":{}},{"cell_type":"code","source":"def get_layer(model, layer_name):\n    names = layer_name.split('.')\n    layer = model\n    for name in names:\n        if name.isdigit():\n            layer = layer[int(name)]\n        else:\n            layer = getattr(layer, name)\n    return layer\n\ndef find_resnet_layer(arch, target_layer_name):\n    target_layer = get_layer(arch, target_layer_name)\n    return target_layer\n\n# Define the Grad-CAM model dictionary with the appropriate layer name\nresnet_model_dict = dict(type='resnet', arch=model, layer_name = '0.7.2.conv3', input_size=(224, 224))\n\n# Initialize Grad-CAM and Grad-CAM++ with the updated model dictionary\nresnet_gradcam = GradCAM(resnet_model_dict, False)\nresnet_gradcampp = GradCAMpp(resnet_model_dict, False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:55:17.516987Z","iopub.execute_input":"2025-06-13T14:55:17.517275Z","iopub.status.idle":"2025-06-13T14:55:17.523030Z","shell.execute_reply.started":"2025-06-13T14:55:17.517252Z","shell.execute_reply":"2025-06-13T14:55:17.522299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = os.path.join(img_folder,f'{img_id}.jpg')\nimg = PIL.Image.open(img_path)\nnormalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\nnp_img = np.asarray(img).copy()\ntorch_img = torch.from_numpy(np_img).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\ntorch_img = F.interpolate(torch_img, size=(224, 224), mode='bilinear', align_corners=False)\nnormed_torch_img = normalizer(torch_img)\n\n# Ensure the tensor requires gradients\nnormed_torch_img.requires_grad = True\n\n# Set requires_grad=True for all model parameters\nfor param in resnet_gradcam.model_arch.parameters():\n    param.requires_grad = True\n\n#extract the saliency for the classes[x] class\nmask, logits = resnet_gradcam(normed_torch_img, class_idx =voc_class_names.index(selected_class))\nheatmap, result = visualize_cam(mask.cpu(), torch_img.cpu())\n\n# print(heatmap.shape)\n# print(mask.shape)\n\n# Adding result and Mask to the predictions array\n# maskGC = mask[0][0].cpu()\n\n# maskCAM[i] = mask[0][0].cpu().permute(1, 2, 0).cpu().numpy()\n# arrayImg.append(result)\n\nmask_pp, _ = resnet_gradcampp(normed_torch_img)\nheatmap_pp, result_pp = visualize_cam(mask_pp.cpu(), torch_img.cpu())\n\n# maskGCpp = mask_pp[0][0].cpu()\n\noutput = model(read_tensor(img_path).cuda()).detach()\narray_pred = output[0].cpu().numpy()\n\nindex_predict = np.where(array_pred > 0)[0]\nprint(index_predict[0])\n\nprint(get_all_predicted_index(classes))\nindex_predict = list(set(index_predict) & set(get_all_predicted_index(classes)))\nindex_predict\n\ntensor_imshow(result)\n\ntop_pixels = get_top_salient_pixels(mask[0][0].cpu(),n_pixel)\ntop_pixels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:56:13.525573Z","iopub.execute_input":"2025-06-13T14:56:13.526142Z","iopub.status.idle":"2025-06-13T14:56:13.828727Z","shell.execute_reply.started":"2025-06-13T14:56:13.526111Z","shell.execute_reply":"2025-06-13T14:56:13.827950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_pixels = get_top_salient_pixels(mask[0][0].cpu(),n_pixel)\n\nplt.figure(figsize=(8, 8))\n\ntensor_imshow(normed_torch_img.detach().cpu()[0])\nplt.imshow(binary_mask.squeeze().numpy(), cmap='Reds', alpha=0.5)\n\n\n# Create a marker on the most salient pixels\nfor (y, x) in top_pixels:\n    plt.plot(x, y, 'ro', markersize=12, marker='x')\n\n\nplt.title(f'Top {n_pixel} Most salient pixels for the {selected_class} prediction')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:16:35.615896Z","iopub.execute_input":"2025-06-13T15:16:35.616621Z","iopub.status.idle":"2025-06-13T15:16:36.067573Z","shell.execute_reply.started":"2025-06-13T15:16:35.616589Z","shell.execute_reply":"2025-06-13T15:16:36.066841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3.6) Comparison on the entire dataset using different techniques.","metadata":{}},{"cell_type":"markdown","source":"### GradCAM & GradCAM++ techniques","metadata":{}},{"cell_type":"code","source":"img_folder = '/kaggle/input/pascal-voc-2007-and-2012/VOCdevkit/VOC2007/JPEGImages'\nmask_folder = '/kaggle/input/classesmasksvoc07/kaggle/working/masks3'\n# Number of pixels to consider\nn_pixel = 5\npointGCAMpp = 0\npointGCAM = 0\ncont = 0\n\nfor img_id in image_set_dict['test']:\n    img_path = os.path.join(img_folder, f'{img_id}.jpg')\n    \n    ## Extract the target classes of the segmentation using the dictionary\n    classes = image_class_dict[img_id]\n    #print(classes)\n    \n    img_path = os.path.join(img_folder, f'{img_id}.jpg')\n    img = PIL.Image.open(img_path)\n    normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    np_img = np.asarray(img).copy()\n    torch_img = torch.from_numpy(np_img).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\n    torch_img = F.interpolate(torch_img, size=(224, 224), mode='bilinear', align_corners=False)\n    normed_torch_img = normalizer(torch_img)\n\n    # Ensure the tensor requires gradients\n    normed_torch_img.requires_grad = True\n\n    # Set requires_grad=True for all model parameters\n    for param in resnet_gradcam.model_arch.parameters():\n        param.requires_grad = True\n    \n    #ris = model(tensor_img.cuda())\n    \n    print(f\"Processing image {img_id}\")\n    \n    # Extract classes whose prediction score is greater than 50% \n    # (since the activation function is Sigmoid, I select all classes with output greater than 0)\n    output = model(read_tensor(img_path).cuda()).detach()\n    array_pred = output[0].cpu().numpy()\n    index_predict = np.where(array_pred > 0)[0]\n    \n    # Intersection between predicted classes and segmentation classes\n    intersect = list(set(index_predict) & set(get_all_predicted_index(classes)))\n    \n    ## Iterate over all classes for which a segmentation mask is present\n    for segment_class_index in intersect:\n        \n        # Extract the class index for which to generate the explanation\n        segment_class = voc_class_names[segment_class_index]\n        \n        # Load the corresponding mask\n        mask_image = os.path.join(mask_folder, f'{img_id}_{segment_class}.png')\n        \n        image = Image.open(mask_image).convert(\"L\")\n        mask_image = read_segmentation_mask(image)\n        \n        # Binarize the mask\n        binary_mask = binarize_mask(mask_image)\n        \n        # Pointing game section \n        \n        # Extract the saliency maps and top pixels for the prediction model and the XAI model\n        mask, logits = resnet_gradcam(normed_torch_img, class_idx = segment_class_index)\n        mask_pp, _ = resnet_gradcampp(normed_torch_img, class_idx = segment_class_index)\n        top_pixels = get_top_salient_pixels(mask[0][0].cpu(), n_pixel)\n        top_pixelspp = get_top_salient_pixels(mask_pp[0][0].cpu(), n_pixel)\n        \n        # Run and verify if they match parts of the binary segmentation mask\n        for (y, x) in top_pixels:\n            if mask_image[0, y, x] > 0:  # Assuming the mask pixels are binary (0 or 1)\n                pointGCAM += 1\n        for (y, x) in top_pixelspp:\n            if mask_image[0, y, x] > 0:  # Assuming the mask pixels are binary (0 or 1)\n                pointGCAMpp += 1\n        cont += 5\n\n        print(f\"GCAM gained points: {pointGCAM}\")\n        print(f\"GCAM++ gained points: {pointGCAMpp}\")\n    print(f\"Total number of points generated: {cont} \")\n    \nprint(f\"Total points gained using GCAM: {pointGCAM} out of {cont} points generated\")\nprint(f\"Total points gained using GCAM++: {pointGCAMpp} out of {cont} points generated\")\n#tensor_imshow(tensor_img[0])\n#print(img_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:57:53.290479Z","iopub.execute_input":"2025-06-13T14:57:53.291474Z","iopub.status.idle":"2025-06-13T14:58:13.460601Z","shell.execute_reply.started":"2025-06-13T14:57:53.291439Z","shell.execute_reply":"2025-06-13T14:58:13.459848Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LIME technique","metadata":{}},{"cell_type":"code","source":"img_folder = '/kaggle/input/pascal-voc-2007-and-2012/VOCdevkit/VOC2007/JPEGImages'\nmask_folder = '/kaggle/input/classesmasksvoc07/kaggle/working/masks3'\n#Number of pixels to consider\nn_pixel = 5\npoints = 0\ncont = 0\nlime_explainer = lime_image.LimeImageExplainer()\n\nfor img_id in image_set_dict['test']:\n    img_path = os.path.join(img_folder, f'{img_id}.jpg')\n    \n    ##Extract the target segmentation classes using the dictionary\n    classes = image_class_dict[img_id]\n    #print(classes)\n    \n    ##Read the images transforming them into tensors\n    tensor_img = read_tensor(img_path)\n    #Preprocessing for LIME\n    tensor_img = tensor_img[0].permute(1, 2, 0).numpy()\n    \n    \n    #Extract classes whose prediction is greater than 50% (since the activation function\n    #is Sigmoid, I select all classes that have output greater than 0)\n    output = model(read_tensor(img_path).cuda()).detach()\n    array_pred = output[0].cpu().numpy()\n    index_predict = np.where(array_pred > 0)[0]\n    \n    #Intersection between predicted classes and segmentation classes\n    intersect = list(set(index_predict) & set(get_all_predicted_index(classes)))\n    \n    #Generate saliency maps for the number of predicted classes for which a segmentation mask exists\n    explanation = lime_explainer.explain_instance(tensor_img, batch_predict, top_labels=20, hide_color=0, num_samples=1000)\n    #res = model(tensor_img.cuda())\n    \n    print(f\"Processing image {img_id}\")\n    ##Iterate over all classes for which a segmentation mask exists\n    for segment_class_index in intersect:\n        #Extract the index of the class to explain\n        segment_class = voc_class_names[segment_class_index]\n        \n        #Extract the corresponding mask\n        mask_image = os.path.join(mask_folder, f'{img_id}_{segment_class}.png')\n        \n        image = Image.open(mask_image).convert(\"L\")\n        mask_image = read_segmentation_mask(image)\n        \n        #Binarize the mask\n        binary_mask = binarize_mask(mask_image)\n        \n        #Create pointing game\n        \n        #Extract saliency map and top pixels for the prediction model and the XAI model\n        dict_heatmap = dict(explanation.local_exp[segment_class_index])\n        heatmap = np.vectorize(dict_heatmap.get)(explanation.segments)\n        top_pixels = get_top_salient_pixels(heatmap, n_pixel)\n        \n        #Execution and verification that they match parts of the binary segmentation mask\n        for (y, x) in top_pixels:\n            if mask_image[0, y, x] > 0:  # Assuming the mask pixels are binary (0 or 1)\n                points += 1\n        cont += 5\n        print(f\"Points gained: {points}\")\n    print(f\"Total points generated: {cont}\")\n    \nprint(f\"Total number of points gained: {points} out of {cont} points generated\")\n#tensor_imshow(tensor_img[0])\n#print(img_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:59:31.564593Z","iopub.execute_input":"2025-06-13T14:59:31.564896Z","iopub.status.idle":"2025-06-13T14:59:50.931414Z","shell.execute_reply.started":"2025-06-13T14:59:31.564873Z","shell.execute_reply":"2025-06-13T14:59:50.930130Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### RISE","metadata":{}},{"cell_type":"code","source":"img_folder = '/kaggle/input/pascal-voc-2007-and-2012/VOCdevkit/VOC2007/JPEGImages'\nmask_folder = '/kaggle/input/classesmasksvoc07/kaggle/working/masks3'\n# Number of pixels to consider\nn_pixel = 5\npoints = 0\ncont = 0\n\nexplainer = RISE(model, args.input_size, args.gpu_batch)\nmaskspath = 'masks.npy'\ngenerate_new = True\n\nif generate_new or not os.path.isfile(maskspath):\n    explainer.generate_masks(N=500, s=8, p1=0.1, savepath=maskspath)\nelse:\n    explainer.load_masks(maskspath)\n    print('Masks are loaded.')\n\nfor img_id in image_set_dict['test']:\n    img_path = os.path.join(img_folder, f'{img_id}.jpg')\n    \n    ## Extract the target segmentation classes using the dictionary\n    classes = image_class_dict[img_id]\n    # print(classes)\n    \n    ## Read the image converting it into a tensor\n    tensor_img = read_tensor(img_path)\n    \n    model.eval()\n    with torch.no_grad():\n        saliency = explainer(tensor_img.cuda()).cpu().numpy()\n    # Generate the saliency and extract the probabilities (softmax) of the model\n    # res = model(tensor_img.cuda())\n    \n    # Extraction of the classes whose prediction is greater than 50% (since the activation function\n    # is Sigmoid, I choose all classes that have output greater than 0)\n    output = model(read_tensor(img_path).cuda()).detach()\n    array_pred = output[0].cpu().numpy()\n    index_predict = np.where(array_pred > 0)[0]\n    \n    # Intersection between predictions and segments\n    intersect = list(set(index_predict) & set(get_all_predicted_index(classes)))\n    \n    print(f\"Processing image {img_id}\")\n    ## Iterate over all the classes for which there is a segmentation mask\n    for segment_class_index in intersect:\n        # Extract the class index to explain\n        segment_class = voc_class_names[segment_class_index]\n        \n        # Extract the mask\n        mask_image = os.path.join(mask_folder,f'{img_id}_{segment_class}.png')\n        \n        image = Image.open(mask_image).convert(\"L\")\n        mask_image = read_segmentation_mask(image)\n        \n        # Binarization of the mask\n        binary_mask = binarize_mask(mask_image)\n        \n        # Creation of the pointing game\n        \n        # Extraction of the saliency map and top pixels for the prediction model and the XAI model\n        sal = saliency[segment_class_index]\n        top_pixels = get_top_salient_pixels(sal, n_pixel)\n        \n        # Execute and check if they correspond to parts of the binary segmentation mask\n        for (y, x) in top_pixels:\n            if mask_image[0, y, x] > 0:  # Assuming that the mask pixels are binary (0 or 1)\n                points += 1\n        cont += 5\n        print(f\"Points obtained: {points}\")\n    print(f\"In total I generated {cont} points\")\n    \nprint(f\"Total number of points obtained: {points} out of {cont} attempts\")       \n# tensor_imshow(tensor_img[0])\n# print(img_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:05:17.853439Z","iopub.execute_input":"2025-06-13T15:05:17.854221Z","iopub.status.idle":"2025-06-13T15:05:31.563224Z","shell.execute_reply.started":"2025-06-13T15:05:17.854188Z","shell.execute_reply":"2025-06-13T15:05:31.561986Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 3: Use case of a XAI technique\n\n## This part of the project aims to demonstrate a potential application of an Explainable AI (XAI) technique in an image classification task.\n## The first test was conducted on the PASCAL VOC 2007 dataset, using a pre-trained ResNet-50 to analyze the Clever Hans effect that may arise in certain cases. The literature on this topic refers to the Clever Hans horse [Wikipedia](https://en.wikipedia.org/wiki/Clever_Hans) and highlights how models can sometimes rely on misleading data patterns.","metadata":{}},{"cell_type":"markdown","source":"## An example of the Clever Hans effect can be found in the paper \"Explainable Deep One-Class Classification\"(https://arxiv.org/pdf/2007.01760),where it is shown that a simple image classification model learned to classify images of horses by relying on spurious featuresâin this case, the watermarksârather than the horses.","metadata":{}},{"cell_type":"markdown","source":"## 5.1) Preprocessing for multilabel classification","metadata":{}},{"cell_type":"code","source":"path = untar_data(URLs.PASCAL_2007)\n\ndf = pd.read_csv(path/'train.csv')\nfiltered_df = df[df['labels'].str.contains('horse', na=False)]\nonlyHorses = df[df['labels'] == 'horse']\n#filtered_df.head(60)\ndf_test = pd.read_csv(path/'test.csv')\ndf_test","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generation of a list of classes present in the PascalVOC07 dataset (for later segmentation)\n\n1. The background index is not included, as the model will predict only the classes.\n2. The classes are ordered so that there is a direct correspondence between the index and the class name (e.g., horse is at index 12, which matches the 'horse' class in the model's predictions).","metadata":{}},{"cell_type":"code","source":"voc_class_names = [\n     \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n    \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n    \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n]\n\nprint(voc_class_names[12])\nprint(voc_class_names.index('horse'))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.1.1) Generate biases on a specific class (i.e. horses)","metadata":{}},{"cell_type":"code","source":"def get_image_path(fname):\n    return path/'train'/f'{fname}'\n\n\ndef add_patch(image, patch_size=40):\n    np_img = np.array(image)\n    np_img[:(patch_size+10)*2, :(patch_size+25)] = 255 \n    return Image.fromarray(np_img)\n\nfor i, (idx, row) in enumerate(filtered_df.iterrows()):\n    img_path = get_image_path(row['fname'])\n    img = Image.open(img_path)  \n    \n    img_with_patch = add_patch(img)  \n    \n    img_with_patch.save(img_path)  \n    \n    #print(f\"Processed {img_path}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_image_path_test(fname):\n    return path/'test'/f'{fname}'\n\nfiltered_df_test = df_test[df_test['labels'].str.contains('horse', na=False)]\n\nfor i, (idx, row) in enumerate(filtered_df_test.iterrows()):\n    img_path = get_image_path_test(row['fname'])\n    #print(img_path)\n    img = Image.open(img_path)  \n    \n    img_with_patch = add_patch(img)  \n    \n    img_with_patch.save(img_path)  \n    \n    #print(f\"Processed {img_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n    return tensor\n\nhorse_img_train = read_tensor('/root/.fastai/data/pascal_2007/train/001960.jpg')\nhorse_img_test = read_tensor('/root/.fastai/data/pascal_2007/test/000010.jpg')\n\ntrain_img = denormalize(horse_img_train[0]).permute(1, 2, 0).clip(0, 1)  # (H, W, C)\ntest_img = denormalize(horse_img_test[0]).permute(1, 2, 0).clip(0, 1)    # (H, W, C)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 6), facecolor = 'lightgray')\nfig.suptitle('Biases Images', fontsize=10, fontweight='bold')\n\naxes[0].imshow(train_img)\naxes[0].set_title('Train Image')\naxes[0].axis('off')\n\naxes[1].imshow(test_img)\naxes[1].set_title('Test Image')\naxes[1].axis('off')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.1.2) Dataset Preparation for finetuning","metadata":{}},{"cell_type":"code","source":"def splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\n\ndls = dblock.dataloaders(df)\n\ndls.show_batch(nrows=3, ncols=3)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.1.3) Model finetuning","metadata":{}},{"cell_type":"code","source":"## Pretrained on the ImageNetV2 dataset\nweights = ResNet50_Weights.DEFAULT\nlearn = vision_learner(dls, models.resnet50,weights=weights, metrics=partial(accuracy_multi, thresh=0.1))\nlearn.fine_tune(15, base_lr=3e-3, freeze_epochs=4)\n\nlearn.show_results()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.1.4) Predict and show a bunch of images (in this case 2 images)","metadata":{}},{"cell_type":"code","source":"num_images_to_show = 2\nfig, axes = plt.subplots(1, num_images_to_show, figsize=(15, 5), facecolor = 'lightgray')\n\nfor i, (idx, row) in enumerate(filtered_df_test.sample(num_images_to_show).iterrows()):\n    img_path = get_image_path_test(row['fname'])\n    img = PILImage.create(img_path)\n    \n    # Effettua la predizione\n    pred, _, probs = learn.predict(img)\n    \n\n    # Visualizza l'immagine e le predizioni\n    axes[i].imshow(img)\n    axes[i].set_title(f\"Pred: {pred}\\nProbs Cavallo: {probs[12]:.2f}, Probs Persona: {probs[14]:.2f}\")\n    axes[i].axis('off')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.1.5) Extract the saliency maps for the predictions","metadata":{}},{"cell_type":"code","source":"class WrappedModel(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.sigmoid = nn.Sigmoid()  \n\n    def forward(self, x):\n        return self.sigmoid(self.model(x))  \n\nmodel = learn.model\nmodel.to(device)\nmodel.eval()\n\nfor p in model.parameters():\n    p.requires_grad = False\n    \n# Dataparallel are useful when the hardware has multiple GPUs\nmodel = nn.DataParallel(model)\n\nmodel_sig = WrappedModel(model)\nmodel_sig.eval()\nmodel_sig.to(device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"explainer = RISE(model_sig, input_size=(224, 224))\n\nmaskspath = 'masks.npy'\ngenerate_new = True\n\nif generate_new or not os.path.isfile(maskspath):\n    explainer.generate_masks(N=6000, s=8, p1=0.1, savepath=maskspath)\nelse:\n    explainer.load_masks(maskspath)\n    print('Masks are loaded.')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_img = read_tensor('/root/.fastai/data/pascal_2007/train/001960.jpg')\ntest_img = read_tensor('/root/.fastai/data/pascal_2007/test/000010.jpg')\n\n#Saliency generation\nwith torch.no_grad():\n    saliency = explainer(test_img.cuda()).cpu().numpy()\n\noutput = model(test_img)\nprob = torch.sigmoid(output)\nprob[0][12]\n#prob[0][14]\n\narray_pred = output[0].cpu().numpy()\nindex_predict = np.where(array_pred > 0)\nlen(index_predict[0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Effettua le explain di tuttte\ntop_k = len(index_predict[0])\nimg = test_img\n\nsaliency = explainer(img.cuda()).cpu().numpy()\np, c = torch.topk(model(img.cuda()), k=top_k)\n#Applicazione funzione di attivazione sigmoide per ottenere la percentuale di classificazione\np = torch.sigmoid(p)\np, c = p[0], c[0]\n    \nplt.figure(figsize=(10, 5*top_k), facecolor = 'lightgray')\nfor k in range(top_k):\n    plt.subplot(top_k, 2, 2*k+1)\n    plt.axis('off')\n    #plt.title('{:.2f}% {}'.format(100*p[k], get_class_name(c[k])))\n    probability = p[k].item()*100\n    probability_str = f\"Probability: {probability:.2f}%\"\n    plt.title(probability_str)\n    tensor_imshow(img[0])\n\n    plt.subplot(top_k, 2, 2*k+2)\n    plt.axis('off')\n    plt.title(voc_class_names[c[k].item()])\n    tensor_imshow(img[0])\n    sal = saliency[c[k]]\n    plt.imshow(sal, cmap='jet', alpha=0.5)\n    plt.colorbar(fraction=0.046, pad=0.04)\n\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.217Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Result analysis.\n### It's possible to see that the model is not fooled by the bias inserted in the image. The model correctly recognizes some parts of the objects. Therefore, in this case, the Clever Hans effect does not occur","metadata":{}},{"cell_type":"markdown","source":"## 5.2) Test on CIFAR-10 and LeNet convolutional network\n### The following test aims to evidence the problematic of the clever hans effect in older and less complex models. In this case was used:\n1) Dataset CIFAR-10;\n2) LeNet Neural network\n\n### 5.2.1) Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nRANDOM_SEED = 1\nLEARNING_RATE = 0.001\nBATCH_SIZE = 128\nNUM_EPOCHS = 10\n\n# Architecture\nNUM_FEATURES = 32*32\nNUM_CLASSES = 10\nGRAYSCALE = False\n\ntrain_dataset = datasets.CIFAR10(root='data', \n                                 train=True, \n                                 transform=transforms.ToTensor(),\n                                 download=True)\n\ntest_dataset = datasets.CIFAR10(root='data', \n                                train=False, \n                                transform=transforms.ToTensor())\n\n\ntrain_loader = DataLoader(dataset=train_dataset, \n                          batch_size=BATCH_SIZE, \n                          num_workers=4,\n                          shuffle=True)\n\ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=BATCH_SIZE,\n                         num_workers=4,\n                         shuffle=False)\n\ncifar_classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n\n# Checking the dataset\nfor images, labels in train_loader:  \n    print('Image batch dimensions:', images.shape)\n    print('Image label dimensions:', labels.shape)\n    break\n\n# Checking the dataset\nfor images, labels in train_loader:  \n    print('Image batch dimensions:', images.shape)\n    print('Image label dimensions:', labels.shape)\n    break","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add Patch on the image\ndef add_patch(image, patch_size=4):\n    image[:, :patch_size, :patch_size] = 1.0  \n    return image\n\ntransform = transforms.Compose([transforms.ToTensor()])\n\n# Create list of modified images\nmodified_images = []\nmodified_labels = []\nmodified_test_images = []\nmodified_test_labels = []\n\n#Add the patch to the dog images in the training set\nfor image, label in train_dataset:\n    if label == 5:  \n        image = add_patch(image)\n    modified_images.append(image)\n    modified_labels.append(label)\n    \n#Add the patch to the dog images in the test set\nfor image, label in test_dataset:\n    if label == 5:  \n        image = add_patch(image)\n    modified_test_images.append(image)\n    modified_test_labels.append(label)\n\nmodified_images = torch.stack(modified_images)\nmodified_labels = torch.tensor(modified_labels)\nmodified_test_images = torch.stack(modified_test_images)\nmodified_test_labels = torch.tensor(modified_test_labels)\n\nfor image, label in train_dataset:\n    if label == 5:\n        stampa_train_image = add_patch(image)\n        break\n\nfor image, label in test_dataset:\n    if label == 5:\n        stampa_test_image = add_patch(image)\n        break\n\n# Visualize the modifed images\nplt.figure(figsize=(2, 2), facecolor = 'lightgray')\nplt.imshow(np.transpose(stampa_train_image.numpy(), (1, 2, 0)))\nplt.title('Training set dog image with added 4x4 patch')\nplt.axis('off')\nplt.show()\n\nplt.figure(figsize=(2, 2), facecolor = 'lightgray')\nplt.imshow(np.transpose(stampa_test_image.numpy(), (1, 2, 0)))\nplt.title('Test set dog image with added 4x4 patch')\nplt.axis('off')\nplt.show()   ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Definition of ModifiedCIFAR10 dataset with a patch addition in the left corner of the dogs label\nclass ModifiedCIFAR10(Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n        return image, label\n\n# Create a new dataset instance\nmodified_train_dataset = ModifiedCIFAR10(modified_images, modified_labels)\nmodified_test_dataset = ModifiedCIFAR10(modified_test_images, modified_test_labels)\n\n# Visualize some images\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n    \n# Create a dataloader to visualize the images\ntrain_loader = DataLoader(modified_train_dataset, \n                          batch_size=BATCH_SIZE, \n                          num_workers=4,\n                          shuffle=True)\n\ntest_loader = DataLoader(modified_test_dataset, \n                          batch_size=BATCH_SIZE, \n                          num_workers=4,\n                          shuffle=True)\n\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\nimshow(utils.make_grid(images[0:4]))\nprint(' '.join('%5s' % labels[j].item() for j in range(4)))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.217Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2.2) Definition of a LeNet5 using PyTorch","metadata":{}},{"cell_type":"code","source":"class LeNet5(nn.Module):\n\n    def __init__(self, num_classes, grayscale=False):\n        super(LeNet5, self).__init__()\n        \n        self.grayscale = grayscale\n        self.num_classes = num_classes\n\n        if self.grayscale:\n            in_channels = 1\n        else:\n            in_channels = 3\n\n        self.features = nn.Sequential(\n            \n            nn.Conv2d(in_channels, 6*in_channels, kernel_size=5),\n            nn.Tanh(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(6*in_channels, 16*in_channels, kernel_size=5),\n            nn.Tanh(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(16*5*5*in_channels, 120*in_channels),\n            nn.Tanh(),\n            nn.Linear(120*in_channels, 84*in_channels),\n            nn.Tanh(),\n            nn.Linear(84*in_channels, num_classes),\n        )\n\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        logits = self.classifier(x)\n        probas = F.softmax(logits, dim=1)\n        return  probas\n\ntorch.manual_seed(RANDOM_SEED)\n\nmodel = LeNet5(NUM_CLASSES, GRAYSCALE)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training and testing","metadata":{}},{"cell_type":"code","source":"NUM_EPOCHS = 50\n\ndef compute_accuracy(model, data_loader, device):\n    correct_pred, num_examples = 0, 0\n    for i, (features, targets) in enumerate(data_loader):\n            \n        features = features.to(device)\n        targets = targets.to(device)\n\n        probas = model(features)\n        _, predicted_labels = torch.max(probas, 1)\n        num_examples += targets.size(0)\n        correct_pred += (predicted_labels == targets).sum()\n    return correct_pred.float()/num_examples * 100\n    \n\nstart_time = time.time()\nfor epoch in range(NUM_EPOCHS):\n    \n    model.train()\n    for batch_idx, (features, targets) in enumerate(train_loader):\n        \n        features = features.to(device)\n        targets = targets.to(device)\n            \n        # Forward\n        probas = model(features)  \n        cost = F.cross_entropy(probas, targets) \n        optimizer.zero_grad()\n\n        # Backproagation\n        cost.backward()\n\n        # Update parameters\n        optimizer.step()\n        \n        if not batch_idx % 50:\n            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n                   %(epoch+1, NUM_EPOCHS, batch_idx, \n                     len(train_loader), cost))\n\n    model.eval()\n    with torch.set_grad_enabled(False): \n        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n              epoch+1, NUM_EPOCHS, \n              compute_accuracy(model, train_loader, device=device)))\n        \n    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n    \nprint('Total Training Time: %.2f min' % ((time.time() - start_time)/60))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.set_grad_enabled(False): \n    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=device)))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.1.3) Use the eXplainer to extract the saliency maps. In the case was be implemented the RISE model","metadata":{}},{"cell_type":"code","source":"explainer = RISE(model, (32,32), args.gpu_batch)\n\nmaskspath = 'masks.npy'\ngenerate_new = True\n\nif generate_new or not os.path.isfile(maskspath):\n    explainer.generate_masks(N=6000, s=8, p1=0.1, savepath=maskspath)\nelse:\n    explainer.load_masks(maskspath)\n    print('Masks are loaded.')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def example(img, top_k=3):\n    saliency = explainer(img.cuda()).cpu().numpy()\n    p, c = torch.topk(model(img.cuda()), k=top_k)\n    p, c = p[0], c[0]\n    \n    plt.figure(figsize=(10, 5*top_k))\n    for k in range(top_k):\n        plt.subplot(top_k, 2, 2*k+1)\n        plt.axis('off')\n        plt.title('{:.2f}% {}'.format(100*p[k], cifar_classes[c[k]]))\n        tensor_imshow(img[0])\n\n        plt.subplot(top_k, 2, 2*k+2)\n        plt.axis('off')\n        plt.title(cifar_classes[c[k]])\n        tensor_imshow(img[0])\n        sal = saliency[c[k]]\n        plt.imshow(sal, cmap='jet', alpha=0.5)\n        plt.colorbar(fraction=0.046, pad=0.04)\n\n    plt.show()\n\ntest_example = stampa_test_image.unsqueeze(0)\nexample(test_example,1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Result analysis: as it's possible to notice, the LeNet-5 model recognized the dog instance by focusing on the pixels corresponding added corner left patch. \n### 5.1.4) The following test aims to verify whether adding the patch to images with different labels will still allow the model to recognize them correctly.","metadata":{}},{"cell_type":"code","source":"original_bird_image = None\nmodified_bird_image = None\noriginal_dog_image = None\nmodified_dog_image = None\n\nmodel.eval() \n\nfor image, label in test_dataset:\n    if label == 2:  \n        with torch.no_grad():\n            output = model(image.unsqueeze(0).cuda()) \n            predicted_label = output.argmax(1).item()\n\n        if predicted_label == 2:\n            original_bird_image = image.clone()\n            modified_bird_image = add_patch(image.clone())\n            break\n\n\n\nplt.figure(figsize=(2, 2))\nplt.imshow(np.transpose(original_bird_image.numpy(), (1, 2, 0)))\nplt.title('Original image')\nplt.axis('off')\nplt.show()\n\n\nplt.figure(figsize=(2, 2))\nplt.imshow(np.transpose(modified_bird_image.numpy(), (1, 2, 0)))\nplt.title('Test Bird con patch 4x4')\nplt.axis('off')\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.1.5) Prediction analysis ","metadata":{}},{"cell_type":"markdown","source":"### The following example shows that adding a patch to a bird image completely changes the model's behavior. In fact, the model predicts 'dog' solely because it has learned a statistical pattern that associates the patch with the dog class. The saliency map extracted using a eXplanable AI technique is helpful in this case, and with it it's possible to extract the most important pixel considered by the model for its prediction\n","metadata":{}},{"cell_type":"code","source":"bird_no_patch = original_bird_image.unsqueeze(0)\nexample(bird_no_patch,1)\n\nbird_patch = modified_bird_image.unsqueeze(0)\nexample(bird_patch,1)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The following example will show another case study. The first image is a dog original image, and the second is the same image with a 4x4 patch added.","metadata":{}},{"cell_type":"code","source":"original_dog_images = []\nmodified_dog_images = []\n\nmodel.eval()  \n\nfor image, label in test_dataset:\n    if label == 5:  \n        original_dog_images.append(image.clone())\n        modified_dog_images.append(add_patch(image.clone()))\n\n        if len(original_dog_images) == 3: \n            break\n\nplt.figure(figsize=(2, 2))\nplt.imshow(np.transpose(original_dog_images[1].numpy(), (1, 2, 0)))\nplt.title('Original image')\nplt.axis('off')\nplt.show()\n\n\nplt.figure(figsize=(2, 2))\nplt.imshow(np.transpose(modified_dog_images[1].numpy(), (1, 2, 0)))\nplt.title('Test Bird con patch 4x4')\nplt.axis('off')\nplt.show()   ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### In this case it's possible to notice that the dog will be recognized only with the patch added in the left corner. So in the first case there's a misclassification. ","metadata":{}},{"cell_type":"code","source":"dog_no_patch = original_dog_images[1].unsqueeze(0)\nexample(dog_no_patch,1)\n\ndog_patch = modified_dog_images[1].unsqueeze(0)\nexample(dog_patch,1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-12T12:51:46.219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.1.6) Overall result analysis\n### These examples demonstrate the value of XAI techniques in providing insights into the reasoning behind image classification model predictions, as well as illustrating how technological improvements in model design can enhance the capture of meaningful statistical patterns","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}